# 一、引言

#### 1.1 监督学习任务（算法）

训练集要求：特征和标签

> - 回归问题：标签可以取任意值，平方误差损失函数来对模型进行评估
>
> - 分类问题：标签的取值个数确定（二项分类和多项分类），交叉熵损失函数来对模型进行评估
>
> - 搜索问题：输出的标签集可能相同，但是标签集内元素排序可能不同（可以为集合中的每个元素分配相应的相关性分数）

#### 1.2 无监督学习任务（算法）

> 仿射变换的特点是通过加权和，对特征进行*线性变换*（linear transformation）， 并通过偏置项来进行*平移*（translation）。
>
> 随机梯度下降算法（优化算法）：在模型训练过程中，不断调整模型参数，使得预测值与真实值的距离最小化（它通过不断地在损失函数递减的方向上更新模型参数来降低误差）。
>
> 可以调整但不在训练过程中更新的参数称为**超参数（hyper parameter）**。
>
> **调参（hyper parameter tuning）**是选择超参数的过程。
>
> **超参数通常是我们根据训练迭代结果来调整的**， 而训练迭代结果是在独立的**验证数据集（validation dataset）**上评估得到的。

# 二、线性神经网络

机器学习模型中的关键要素是**训练数据、损失函数、优化算法，还有模型本身**。

在初始化参数之后，我们的任务是**更新这些参数**，直到这些参数足够拟合我们的数据。 每次更新都需要**计算损失函数关于模型参数的梯度**。 有了这个梯度，我们就可以向**减小损失的方向**更新每个参数。

<font color = green>线性回归</font>


> 对于线性回归，每个输入都与每个输出（在本例中只有一个输出）相连， 我们将这种变换称为*全连接层*（fully-connected layer）或称为*稠密层*（dense layer）。
>
> <center>
>   <img src = "https://zh.d2l.ai/_images/singleneuron.svg" width = "70%">
> </center>

<font color = green>`softmax`回归</font>


> <center>
>   <img src = "https://zh.d2l.ai/_images/softmaxreg.svg" width = "80%">
> </center>
>
> - `softmax`运算获取一个向量并将其映射为概率。
> - `softmax`回归适用于分类问题，它使用了`softmax`运算中输出类别的概率分布。
> - 交叉熵是一个衡量两个概率分布之间差异的很好的度量，它测量给定模型编码数据所需的比特数。
>
> 分类问题最常用的损失函数之一：*交叉熵损失*（cross-entropy loss）。
>
> - 借助 `softmax` 回归，我们可以训练多分类的模型。
> - 训练 `softmax` 回归循环模型与训练线性回归模型非常相似：先读取数据，再定义模型和损失函数，然后使用优化算法训练模型。大多数常见的深度学习模型都有类似的训练过程。

**信息论（information theory）**涉及编码、解码、发送以及尽可能简洁地处理信息或数据。

# 三、多层感知机

#### 3.1 激活函数

**激活函数（activation function）**通过计算加权和并加上偏置来确定神经元是否应该被激活， 它们将输入信号转换为输出的可微运算。 大多数激活函数都是非线性的。 由于激活函数是深度学习的基础，下面简要介绍一些常见的激活函数。

> 最受欢迎的激活函数是**修正线性单元（Rectified linear unit，`ReLU`）**， 因为它实现简单，同时在各种预测任务中表现良好。 `ReLU`提供了一种非常简单的非线性变换。 给定元素 $x$ ，`ReLU` 函数被定义为该元素与 0 的最大值。
>
> 通俗地说，`ReLU` 函数通过将相应的活性值设为 0，仅保留正元素并丢弃所有负元素。 为了直观感受一下，我们可以画出函数的曲线图。 正如从图中所看到，激活函数是分段线性的。
>
> <center>
>   <img src = "https://zh.d2l.ai/_images/output_mlp_76f463_21_0.svg" width = "60%">
> </center>

#### 3.2 模型过拟合

介绍模型过拟合之前，先说说什么是模型误差：

> <font color = green>模型误差分为两种：</font>
>
> **训练误差（training error）**是指， 模型在训练数据集上计算得到的误差。
>
> **泛化误差（generalization error）**是指， 模型应用在，同样从原始样本分布中抽取无限多的数据样本时，模型误差的期望。

将模型在训练数据上拟合的比在潜在分布中更接近的现象，称为**过拟合（over - fitting）**， 用于对抗过拟合的技术称为**正则化（regularization）**。

> <font color = green>正则化：防止模型过拟合</font>
>
> **权重衰减（weight decay）**
>
> - 正则化是处理过拟合的常用方法：在训练集的损失函数中加入惩罚项，以降低学习到的模型的复杂度。
> - 保持模型简单的一个特别的选择是使用 $L2$ 惩罚的权重衰减。这会导致学习算法更新步骤中的权重衰减。
> - 权重衰减功能在深度学习框架的优化器中提供。
> - 在同一训练代码实现中，不同的参数集可以有不同的更新行为。
>
> **暂退法（Dropout）**
>
> - 暂退法在前向传播过程中，计算每一内部层的同时丢弃一些神经元。
> - 暂退法可以避免过拟合，它通常与控制权重向量的维数和大小结合使用的。
> - 暂退法将活性值 $h$ 替换为具有期望值 $h$ 的随机变量。
> - 暂退法仅在训练期间使用。

#### 3.3 向前传播

**前向传播（forward propagation或forward pass）** 指的是：按顺序（从输入层到输出层）计算和存储神经网络中每层的结果。

> <font color = green>向前传播计算图</font>
>
> <center>
><img src = "https://zh.d2l.ai/_images/forward.svg" width = "90%">
> </center>
>   
> 正方形表示变量，圆圈表示操作符。 左下角表示输入，右上角表示输出。 注意显示数据流的箭头方向主要是向右和向上的。

#### 3.4 反向传播

**反向传播（backward propagation 或 back propagation）**指的是计算神经网络参数梯度的方法。 简言之，该方法根据微积分中的*链式规则*，按相反的顺序从输出层到输入层遍历网络。 该算法存储了计算某些参数梯度时所需的任何中间变量（偏导数）。

训练神经网络时，在初始化模型参数后， 我们交替使用前向传播和反向传播，利用反向传播给出的梯度来更新模型参数。 注意，反向传播重复利用前向传播中存储的中间值，以避免重复计算。

# 四、深度学习计算

#### 4.1 层和块

> - 一个块可以由许多层组成；一个块可以由许多块组成。
> - 块可以包含代码。
> - 块负责大量的内部处理，包括参数初始化和反向传播。
> - 层和块的顺序连接由`Sequential`块处理。

#### 4.3 延后初始化

> - 延后初始化使框架能够自动推断参数形状，使修改模型架构变得容易，避免了一些常见的错误。
> - 我们可以通过模型传递数据，使框架最终初始化参数。

# 五、卷积神经网络

#### 5.1 卷积层

> 当图像处理的局部区域很小时，卷积神经网络与多层感知机的训练差异可能是巨大的：以前，多层感知机可能需要数十亿个参数来表示网络中的一层，而现在卷积神经网络通常只需要几百个参数，而且不需要改变输入或隐藏表示的维数。 参数大幅减少的代价是，我们的特征现在是平移不变的，并且当确定每个隐藏活性值时，每一层只包含局部的信息。 以上所有的权重学习都将依赖于归纳偏置。当这种偏置与现实相符时，我们就能得到样本有效的模型，并且这些模型能很好地泛化到未知数据中。 但如果这偏置与现实不符时，比如当图像不满足平移不变时，我们的模型可能难以拟合我们的训练数据。
>
> - 图像的平移不变性使我们以相同的方式处理局部图像，而不在乎它的位置。
> - 局部性意味着计算相应的隐藏表示只需一小部分局部图像像素。
> - 在图像处理中，卷积层通常比全连接层需要更少的参数，但依旧获得高效用的模型。
> - 卷积神经网络（CNN）是一类特殊的神经网络，它可以包含多个卷积层。
> - 多个输入和输出通道使模型在每个空间位置可以获取图像的多方面特征。

#### 5.2 图像卷积

> 图像卷积本质就是互相关运算
>
> <center>
>   <img src = "https://zh.d2l.ai/_images/correlation.svg" width = "70%">
> </center>
>
> 在二维互相关运算中，卷积窗口从输入张量的左上角开始，从左到右、从上到下滑动。
>
> 注意，输出大小略小于输入大小。这是因为卷积核的宽度和高度大于1， 而卷积核只与图像中每个大小完全适合的位置进行互相关运算。 所以，输出大小等于输入大小 $n_{h}$ × $n_{w}$ 减去卷积核大小 $k_{h}$ × $k_{w}$，即：
> $$
> \left ( n_{h} - k_{h} + 1 \right ) \times \left ( n_{w} - k_{w} + 1 \right)
> $$
> 这是因为我们需要足够的空间在图像上“移动”卷积核。
>
> <font color = green>小结</font>
>
> - 二维卷积层的核心计算是二维互相关运算。最简单的形式是，对二维输入数据和卷积核执行互相关操作，然后添加一个偏置。
> - 我们可以设计一个卷积核来检测图像的边缘。
> - 我们可以从数据中学习卷积核的参数。
> - 学习卷积核时，无论用严格卷积运算或互相关运算，卷积层的输出不会受太大影响。
> - 当需要检测输入特征中更广区域时，我们可以构建一个更深的卷积网络。

#### 5.3 填充和步幅

> <font color = green>填充</font>
>
> 在应用多层卷积时，我们常常丢失边缘像素。 由于我们通常使用小卷积核，因此对于任何单个卷积，我们可能只会丢失几个像素。 但随着我们应用许多连续卷积层，累积丢失的像素数就多了。 解决这个问题的简单方法即为**填充（padding）**：在输入图像的边界填充元素（通常填充元素是 0 ）。如下图，我们将 3 × 3 输入填充到 5 × 5，那么它的输出就增加为 4 × 4。
>
> <center>
>   <img src = "https://zh.d2l.ai/_images/conv-pad.svg" width = "70%">
> </center>
>
> 通常，如果我们添加 $p_{h}$ 行填充（大约一半在顶部，一半在底部）和 $p_{w}$ 列填充（左侧大约一半，右侧一半），则输出形状将为：
> $$
> \left ( n_{h} - k_{h} + p_{h} + 1 \right ) \times \left ( n_{w} - k_{w} + p_{w} + 1 \right)
> $$
> 在许多情况下，我们需要设置 $ p_{h} = k_{h} − 1$ 和 $p_{w} = k_{w} − 1$，使输入和输出具有相同的形状（通常，卷积核为奇数宽高）。
>
> <font color = green>步幅</font>
>
> 在计算互相关时，卷积窗口从输入张量的左上角开始，向下、向右滑动。 在前面的例子中，我们默认每次滑动一个元素。 但是，有时候为了高效计算或是缩减采样次数，卷积窗口可以跳过中间位置，每次滑动多个元素。

# 六、现代卷积神经网络

#### 6.1 深度卷积神经网络

> - AlexNet 的架构与 LeNet 相似，但使用了更多的卷积层和更多的参数来拟合大规模的 ImageNet 数据集。
> - 今天，AlexNet 已经被更有效的架构所超越，但它是从浅层网络到深层网络的关键一步。
> - 尽管 AlexNet 的代码只比 LeNet 多出几行，但学术界花了很多年才接受深度学习这一概念，并应用其出色的实验结果。这也是由于缺乏有效的计算工具。
> - Dropout、ReLU 和预处理是提升计算机视觉任务性能的其他关键步骤。
>
> <center>
>   <img src = "https://zh.d2l.ai/_images/alexnet.svg" width = "80%">
> </center>

#### 6.2 使用块的网络

> - VGG-11使用可复用的卷积块构造网络。不同的VGG模型可通过每个块中卷积层数量和输出通道数量的差异来定义。
> - 块的使用导致网络定义的非常简洁。使用块可以有效地设计复杂的网络。
> - 在 VGG 论文中，Simonyan和 Ziserman 尝试了各种架构。特别是他们发现深层且窄的卷积（即3×3）比较浅层且宽的卷积更有效。
>
> <center>
>   <img src = "https://zh.d2l.ai/_images/vgg.svg" width = "90%">
> </center>

#### 6.3 网络中的网络（NiN）

> - NiN使用由一个卷积层和多个 $1×1$ 卷积层组成的块。该块可以在卷积神经网络中使用，以允许更多的每像素非线性。
> - NiN去除了容易造成过拟合的全连接层，将它们替换为全局平均汇聚层（即在所有位置上进行求和）。该汇聚层通道数量为所需的输出数量（例如，Fashion-MNIST的输出为10）。
> - 移除全连接层可减少过拟合，同时显著减少NiN的参数。
> - NiN的设计影响了许多后续卷积神经网络的设计。
>
> <center>
>   <img src = "https://zh.d2l.ai/_images/nin.svg" width = "90%">
> </center>

#### 6.4 含并行连结的网络（GoogLeNet）

##### 6.4.1 Inception 块

> Inception块由四条并行路径组成。 前三条路径使用窗口大小为 1×1、3×3和5×5的卷积层，从不同空间大小中提取信息。 中间的两条路径在输入上执行1×1卷积，以减少通道数，从而降低模型的复杂性。 第四条路径使用3×3最大汇聚层，然后使用1×1卷积层来改变通道数。 这四条路径都使用合适的填充来使输入与输出的高和宽一致，最后我们将每条线路的输出在通道维度上连结，并构成Inception块的输出。在Inception块中，通常调整的超参数是每层输出通道数。
>
> <center>
>   <img src = "https://zh.d2l.ai/_images/inception.svg" width = "90%">
> </center>

##### 6.4.2 GoogLeNet 模型

> GoogLeNet一共使用 9 个 Inception 块和全局平均汇聚层的堆叠来生成其估计值。Inception 块之间的最大汇聚层可降低维度。 第一个模块类似于 AlexNet 和 LeNet，Inception 块的组合从 VGG 继承，全局平均汇聚层避免了在最后使用全连接层。
>
> <center>
>   <img src = "https://zh.d2l.ai/_images/inception-full.svg" width = "30%">
> </center>
><font color = green>小结</font>
> 
>- Inception块相当于一个有 4 条路径的子网络。它通过不同窗口形状的卷积层和最大汇聚层来并行抽取信息，并使用 $1 × 1$ 卷积层减少每像素级别上的通道维数从而降低模型复杂度。
> - GoogLeNet将多个设计精细的 Inception 块与其他层（卷积层、全连接层）串联起来。其中 Inception 块的通道数分配之比是在 ImageNet 数据集上通过大量的实验得来的。
> - GoogLeNet 和它的后继者们一度是 ImageNet 上最有效的模型之一：它以较低的计算复杂度提供了类似的测试精度。

#### 6.5 批量规范化

> 批量规范化应用于单个可选层（也可以应用到所有层），其原理如下：在每次训练迭代中，我们首先规范化输入，即通过减去其均值并除以其标准差，其中两者均基于当前小批量处理。 接下来，我们应用比例系数和比例偏移。 正是由于这个基于*批量*统计的*标准化*，才有了*批量规范化*的名称。
>
> 只有使用足够大的小批量，批量规范化这种方法才是有效且稳定的。在应用批量规范化时，批量大小的选择可能比没有批量规范化时更重要。
>
> 由于单位方差是一个主观的选择，因此我们通常包含*拉伸参数*（scale）γ和*偏移参数*（shift）β，它们的形状与输入矩阵 $x$ 相同。 请注意，γ 和 β 是需要与其他模型参数一起学习的参数。
>
> - 通常，我们将批量规范化层置于全连接层中的仿射变换和激活函数之间。
> - 通常，我们将批量规范化层置于全连接层中的仿射变换和激活函数之间。
>
> <font color = green>小结</font>
>
> - 在模型训练过程中，批量规范化利用小批量的均值和标准差，不断调整神经网络的中间输出，使整个神经网络各层的中间输出值更加稳定。
> - 批量规范化在全连接层和卷积层的使用略有不同。
> - 批量规范化层和暂退层一样，在训练模式和预测模式下计算不同。
> - 批量规范化有许多有益的副作用，主要是正则化（防止模型过拟合）。另一方面，”减少内部协变量偏移“的原始动机似乎不是一个有效的解释。

#### 6.6 残差网络（ResNet）

> 残差网络的核心思想是：每个附加层都应该更容易地包含原始函数作为其元素之一。 于是，*残差块*（residual blocks）便诞生了，这个设计对如何建立深层神经网络产生了深远的影响。 凭借它，ResNet 赢得了 2015 年 ImageNet 大规模视觉识别挑战赛。

##### 6.6.1 残差块

> 左图虚线框中的部分需要直接拟合出该映射 $f(x)$，而右图虚线框中的部分则需要拟合出残差映射 $f(x)−x$。 残差映射在现实中往往更容易优化。 实际中，当理想映射 $f(x)$ 极接近于恒等映射时，残差映射也易于捕捉恒等映射的细微波动。是     ResNet 的基础架构 *残差块（residual block）*。 在残差块中，输入可通过跨层数据线路更快地向前传播。
>
> <center>
>   <img src = "https://zh.d2l.ai/_images/residual-block.svg" width = "80%">
> </center>
>
> 

##### 6.6.2 ResNet 模型

<center>
  <img src = "https://zh.d2l.ai/_images/resnet18.svg" width = "30%">
</center>

##### 6.6.3 小结

> - 学习嵌套函数（nested function）是训练神经网络的理想情况。在深层神经网络中，学习另一层作为恒等映射（identity function）较容易（尽管这是一个极端情况）。
> - 残差映射可以更容易地学习同一函数，例如将权重层中的参数近似为零。
> - 利用残差块（residual blocks）可以训练出一个有效的深层神经网络：输入可以通过层间的残余连接更快地向前传播。
> - 残差网络（ResNet）对随后的深层神经网络设计产生了深远影响。

#### 6.7 稠密连接网络（DenseNet）

> ResNet 将 $f(x)$ 分解为两部分：一个简单的线性项和一个复杂的非线性项。
> $$
> f(x) = x + g(x)
> $$
> 那么再向前拓展一步，如果我们想将f拓展成超过两部分的信息呢？ 一种方案便是DenseNet，如下图所示。ResNet 和 DenseNet 的关键区别在于，DenseNet输出是*连接*（用图中的 $[,]$ 表示）而不是如 ResNet 的简单相加。 
>
> <center>
>   <img src = "https://zh.d2l.ai/_images/densenet-block.svg" width = "50%">
> </center>
>
> DenseNet 网络实现起来非常简单：我们不需要添加术语，而是将它们连接起来。 DenseNet 这个名字由变量之间的“稠密连接”而得来，最后一层与之前的所有层紧密相连。 稠密连接如下所示。
>
> <center>
>   <img src = "https://zh.d2l.ai/_images/densenet.svg" width = "60%">
> </center>
>
> <font color = green>总结</font>
>
> - 在跨层连接上，不同于 ResNet 中将输入与输出相加，稠密连接网络（DenseNet）在通道维上连结输入与输出。
> - DenseNet 的主要构建模块是稠密块和过渡层。
> - 在构建 DenseNet 时，我们需要通过添加过渡层来控制网络的维数，从而再次减少通道的数量。

# 七、循环神经网络

#### 7.1 序列模型

> - 内插法（在现有观测值之间进行估计）和外推法（对超出已知观测范围进行预测）在实践的难度上差别很大。因此，对于所拥有的序列数据，在训练时始终要尊重其时间顺序，即最好不要基于未来的数据进行训练。
> - 序列模型的估计需要专门的统计工具，两种较流行的选择是自回归模型和隐变量自回归模型。
> - 对于时间是向前推进的因果模型，正向估计通常比反向估计更容易。
> - 对于直到时间步 $t$ 的观测序列，其在时间步 $t+k$ 的预测输出是 “ $k$ 步预测 ”。随着我们对预测时间k值的增加，会造成误差的快速累积和预测质量的极速下降。

#### 7.2 循环神经网络（RNN）

> 隐藏层：在循环神经网络（RNN）中，隐藏层扮演着至关重要的角色。它不仅接收来自输入层的信息，还接收来自前一时间步的隐藏状态信息。这种设计允许 RNN 捕捉序列数据中的时间依赖性或上下文信息。
>
> 在每个时间步 $t$，RNN 的隐藏层执行以下操作：
>
> - 接收输入：从输入层获取当前时间步的输入向量 $x_{t}$。
> - 接收上一时刻的状态：从上一个时间步 $t-1$ 获取隐藏状态 $h_{t-1}$。
> - 计算新的隐藏状态：通过线性组合和非线性激活函数来更新隐藏状态 $h_{t}$。
>
> $$
> h_{t} = \phi(W_{xh} \cdot x_{t} + W_{hh} \cdot h_{t-1} + b_{h})
> $$
>
> - $W_{xh}$ 是从输入到隐藏层的权重矩阵
> - $W_{hh}$ 是隐藏层到自身的权重矩阵
> - $b_{h}$ 是偏置项
> - $\phi$ 是激活函数，如 $tanh$ 或 $ReLU$

# 八、现代循环神经网络

#### 8.1 门控循环单元

> <center>
>   <img src = "https://zh.d2l.ai/_images/gru-3.svg" width = "80%">
> </center>
>
> - 门控循环神经网络可以更好地捕获时间步距离很长的序列上的依赖关系。
> - 重置门有助于捕获序列中的短期依赖关系。
> - 更新门有助于捕获序列中的长期依赖关系。
> - 重置门打开时，门控循环单元包含基本循环神经网络；更新门打开时，门控循环单元可以跳过子序列。

#### 8.2 长短期记忆网络

> <center>
>   <img src = "https://zh.d2l.ai/_images/lstm-3.svg" width = "90%">
> </center>
>
> - 长短期记忆网络有三种类型的门：输入门、遗忘门和输出门。
> - 长短期记忆网络的隐藏层输出包括“隐状态”和“记忆元”。只有隐状态会传递到输出层，而记忆元完全属于内部信息。
> - 长短期记忆网络可以缓解梯度消失和梯度爆炸。

#### 8.3 深度循环神经网络

> 下图描述了一个具有 $L$ 个隐藏层的深度循环神经网络， 每个隐状态都连续地传递到当前层的下一个时间步和下一层的当前时间步（隐藏层随时被更新）。
>
> <center>
>   <img src = "https://zh.d2l.ai/_images/deep-rnn.svg" width = "50%">
> </center>
>
> 其中，$L$ 表示隐藏层的个数，$T$ 表示时间步。
>
> <font color = green>小结</font>
>
> - 在深度循环神经网络中，隐状态的信息被传递到当前层的下一时间步和下一层的当前时间步。
> - 有许多不同风格的深度循环神经网络， 如长短期记忆网络、门控循环单元、或经典循环神经网络。 这些模型在深度学习框架的高级API中都有涵盖。
> - 总体而言，深度循环神经网络需要大量的调参（如学习率和修剪） 来确保合适的收敛，模型的初始化也需要谨慎。

#### 8.4 双向循环神经网络

> 如果我们希望在循环神经网络中拥有一种机制， 使之能够提供与隐马尔可夫模型类似的前瞻能力， 我们就需要修改循环神经网络的设计。 幸运的是，这在概念上很容易， 只需要增加一个“从最后一个词元开始从后向前运行”的循环神经网络， 而不是只有一个在前向模式下 “从第一个词元开始运行” 的循环神经网络。**双向循环神经网络（bidirectional RNNs）**添加了反向传递信息的隐藏层，以便更灵活地处理此类信息。下图描述了具有单个隐藏层的双向循环神经网络架构。
>
> <center>
>   <img src = "https://zh.d2l.ai/_images/birnn.svg" width = "65%">
> </center>
>
> <font color = green>小结</font>
>
> - 在双向循环神经网络中，每个时间步的隐状态由当前时间步的前后数据同时决定。
> - 双向循环神经网络与概率图模型中的“前向-后向”算法具有相似性。
> - 双向循环神经网络主要用于序列编码和给定双向上下文的观测估计。
> - 由于梯度链更长，因此双向循环神经网络的训练代价非常高。

#### 8.5 编码器 - 解码器架构

> 机器翻译是序列转换模型的一个核心问题， 其输入和输出都是长度可变的序列。 为了处理这种类型的输入和输出， 我们可以设计一个包含两个主要组件的架构： 第一个组件是一个*编码器*（encoder）： 它接受一个长度可变的序列作为输入， 并将其转换为具有固定形状的编码状态。 第二个组件是*解码器*（decoder）： 它将固定形状的编码状态映射到长度可变的序列。 这被称为*编码器-解码器（encoder-decoder）*架构， 如下图所示。
>
> <center>
>   <img src = "https://zh.d2l.ai/_images/encoder-decoder.svg" width = "70%">
> </center>
><font color = green>小结</font>
> 
>- **编码器－解码器架构**可以将长度可变的序列作为输入和输出，因此适用于机器翻译等序列转换问题。
> - 编码器将长度可变的序列作为输入，并将其转换为具有固定形状的编码状态。
> - 解码器将具有固定形状的编码状态映射为长度可变的序列。

#### 8.6 序列到序列的学习

> - 根据“编码器-解码器”架构的设计， 我们可以使用两个循环神经网络来设计一个序列到序列学习的模型。
> - 在实现编码器和解码器时，我们可以使用多层循环神经网络。
> - 我们可以使用遮蔽来过滤不相关的计算，例如在计算损失时。
> - 在编码器－解码器训练中，强制教学方法将原始输出序列（而非预测结果）输入解码器。
> - BLEU 是一种常用的评估方法，它通过测量预测序列和标签序列之间的n元语法的匹配度来评估预测。

#### 8.7 束搜索

> - 序列搜索策略包括贪心搜索、穷举搜索和束搜索。
> - 贪心搜索所选取序列的计算量最小，但精度相对较低。
> - 穷举搜索所选取序列的精度最高，但计算量最大。
> - 束搜索通过灵活选择束宽，在正确率和计算代价之间进行权衡。

# 九、注意力机制

#### 9.1 注意力提示

> - 人类的注意力是有限的、有价值和稀缺的资源。
> - 受试者使用非自主性和自主性提示有选择性地引导注意力。前者基于突出性，后者则依赖于意识。
> - 注意力机制与全连接层或者汇聚层的区别源于增加的自主提示。
> - 由于包含了自主性提示，注意力机制与全连接的层或汇聚层不同。
> - 注意力机制通过注意力汇聚使选择偏向于值（感官输入），其中包含查询（自主性提示）和键（非自主性提示）。键和值是成对的。
> - 可视化查询和键之间的注意力权重是可行的。

#### 9.2 注意力汇聚：`Nadaraya-Watson` 核回归

> - `Nadaraya-Watson` 核回归是具有注意力机制的机器学习范例。
> - `Nadaraya-Watson` 核回归的注意力汇聚是对训练数据中输出的加权平均。从注意力的角度来看，分配给每个值的注意力权重取决于将值所对应的键和查询作为输入的函数。
> - 注意力汇聚可以分为非参数型和带参数型。

#### 9.3 注意力评分函数

> - 将注意力汇聚的输出计算可以作为值的加权平均，选择不同的注意力评分函数会带来不同的注意力汇聚操作。
> - 当查询和键是不同长度的矢量时，可以使用可加性注意力评分函数。当它们的长度相同时，使用缩放的“点－积”注意力评分函数的计算效率更高。

#### 9.4 `Bahdanau` 注意力

> - 在预测词元时，如果不是所有输入词元都是相关的，那么具有 `Bahdanau` 注意力的循环神经网络编码器-解码器会有选择地统计输入序列的不同部分。这是通过将上下文变量视为加性注意力池化的输出来实现的。
> - 在循环神经网络编码器-解码器中，`Bahdanau` 注意力将上一时间步的解码器隐状态视为查询，在所有时间步的编码器隐状态同时视为键和值。

#### 9.5 多头注意力

> <center>
>   <img src = "https://zh.d2l.ai/_images/multi-head-attention.svg" width = "60%">
> </center>
>
> - 多头注意力融合了来自于多个注意力汇聚的不同知识，这些知识的不同来源于相同的查询、键和值的不同的子空间表示。
> - 基于适当的张量操作，可以实现多头注意力的并行计算。

#### 9.6 自注意力和位置编码

> - 在自注意力中，查询、键和值都来自同一组输入。
> - 卷积神经网络和自注意力都拥有并行计算的优势，而且自注意力的最大路径长度最短。但是因为其计算复杂度是关于序列长度的二次方，所以在很长的序列中计算会非常慢。
> - 为了使用序列的顺序信息，可以通过在输入表示中添加位置编码，来注入绝对的或相对的位置信息。

#### 9.7 Transformer

> 下图中概述了 Transformer 的架构。
>
> <center>
>   <img src = "https://zh.d2l.ai/_images/transformer.svg" width = "70%">
> </center>
>
> 从宏观角度来看，Transformer 的**编码器**是由多个相同的层叠加而成的，每个层都有两个子层（子层表示为sublayer）。第一个子层是*多头自注意力*（multi-head self-attention）汇聚；第二个子层是*基于位置的前馈网络*（position wise feed-forward network）。具体来说，在计算编码器的自注意力时，查询、键和值都来自前一个编码器层的输出。受残差网络的启发，每个子层都采用了*残差连接*（residual connection）。
>
> Transformer解码器也是由多个相同的层叠加而成的，并且层中使用了残差连接和层规范化。除了编码器中描述的两个子层之外，解码器还在这两个子层之间插入了第三个子层，称为*编码器－解码器注意力*（encoder-decoder attention）层。在编码器－解码器注意力中，查询来自前一个解码器层的输出，而键和值来自整个编码器的输出。在解码器自注意力中，查询、键和值都来自上一个解码器层的输出。
>
> <font color = green>总结</font>
>
> - Transformer是编码器－解码器架构的一个实践，尽管在实际情况中编码器或解码器可以单独使用。
> - 在Transformer中，多头自注意力用于表示输入序列和输出序列，不过解码器必须通过掩蔽机制来保留自回归属性。
> - Transformer中的残差连接和层规范化是训练非常深度模型的重要工具。
> - Transformer模型中基于位置的前馈网络使用同一个多层感知机，作用是对所有序列位置的表示进行转换。
